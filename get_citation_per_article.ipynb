{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_idgs = pd.read_excel('<your data dosen path in excel>')\n",
    "df_samples = df_idgs.sample(3)\n",
    "df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get publication data\n",
    "def get_publication_data(scholar_id, author_name, delay_times=1.0):\n",
    "    url = f'https://scholar.google.com/citations?user={scholar_id}&hl=en'\n",
    "    browser.get(url)\n",
    "    logging.info(f'Getting publication data for scholar_id: {scholar_id} - {author_name}')\n",
    "    time.sleep(delay_times)\n",
    "\n",
    "    # Click the \"Show more\" button\n",
    "    while True:\n",
    "        try:\n",
    "            show_more_button = browser.find_element('id', 'gsc_bpf')\n",
    "            if show_more_button.is_displayed():\n",
    "                show_more_button.click()\n",
    "                logging.info('Clicking the \"Show more\" button')\n",
    "                time.sleep(delay_times)\n",
    "                break\n",
    "            else:\n",
    "                logging.info('No more \"Show more\" button')\n",
    "                break\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    publications = []\n",
    "    for row in soup.find_all('tr', class_='gsc_a_tr'):\n",
    "        title_elem = row.find('a', class_='gsc_a_at')\n",
    "        title = row.find('a', class_='gsc_a_at').text\n",
    "        year = row.find('span', class_='gsc_a_hc').text\n",
    "        citations = row.find('a', class_='gsc_a_ac').text\n",
    "        authors_publisher = row.find_all('div', class_='gs_gray')\n",
    "        authors = authors_publisher[0].text\n",
    "        publisher = authors_publisher[1].text\n",
    "\n",
    "        article_url = title_elem['href'] if title_elem else 'N/A'\n",
    "        article_id = article_url.split('citation_for_view=')[1] if article_url != 'N/A' else 'N/A'\n",
    "\n",
    "        publications.append({\n",
    "            'scholar_id': scholar_id,\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'publisher': publisher,\n",
    "            'year': year,\n",
    "            'citations': citations,\n",
    "            'article_url': article_url,\n",
    "            'article_id': article_id,\n",
    "        })\n",
    "        logging.info(f'Append publication data: {publications[-1][\"title\"]}')\n",
    "\n",
    "    return publications\n",
    "\n",
    "# Not stable yet, because the we need a scheduling mechanism to avoid the blocking of the data.\n",
    "# Commented the function below.\n",
    "# def fetch_publication_data(scholar_ids):\n",
    "#     publications = []\n",
    "#     with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#         futures = [executor.submit(get_publication_data, scholar_id) for scholar_id in scholar_ids]\n",
    "#         for future in as_completed(futures):\n",
    "#             publications.extend(future.result())\n",
    "#     return publications\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "\n",
    "# Initialize the WebDriver\n",
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# DataFrame to store the results\n",
    "columns = ['scholar_id', 'title', 'authors', 'publisher', 'year', 'citations', 'article_url', 'article_id']\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Loop through each scholar_id and get the publication data\n",
    "# Change the `df_idgs` to `df_samples` to test the function for fewer samples\n",
    "for index, row in df_samples.iterrows():\n",
    "    scholar_id = row['scholar_id']\n",
    "    author_name = row['NAMA']\n",
    "    publications = get_publication_data(scholar_id, author_name)\n",
    "    results_df = pd.concat([results_df, pd.DataFrame(publications)], ignore_index=True)\n",
    "    logging.info(f'Index-{index}: Finished getting publication data for scholar_id: {scholar_id} - {author_name}')\n",
    "\n",
    "# Uncomment the below code if you commented the `fetch_publication_data` function\n",
    "# scholar_ids = df_samples['scholar_id'].tolist()\n",
    "# publications = fetch_publication_data(scholar_ids)\n",
    "# df_publications = pd.DataFrame(publications)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "# df_publications.to_csv('publications.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "browser.quit()\n",
    "\n",
    "# Display the results\n",
    "results_df.head()\n",
    "# results_df.to_csv('publications.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citation_data(article_id, article_url, delay_times=2.0):\n",
    "\turl = f'https://scholar.google.com{article_url}'\n",
    "\ttry:\n",
    "\t\tbrowser.get(url)\n",
    "\t\ttime.sleep(delay_times)  # Wait for the page to load\n",
    "\t\tlogging.info(f'Getting citation data for article ID {article_id} - URL: {url}')\n",
    "\t\thtml = browser.page_source\n",
    "\t\tsoup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\t\tcitations_per_year = {}\n",
    "\n",
    "\t\t# Find the graph wrapper\n",
    "\t\tgraph_bars = soup.find('div', id='gsc_oci_graph_bars')\n",
    "\t\tif not graph_bars:\n",
    "\t\t\tprint(f\"No graph bars found for {url}\")\n",
    "\t\t\treturn citations_per_year\n",
    "\n",
    "\t\t# Extract year labels and citation counts\n",
    "\t\tyears = graph_bars.find_all('span', class_='gsc_oci_g_t')\n",
    "\t\tcitation_bars = graph_bars.find_all('a', class_='gsc_oci_g_a')\n",
    "\n",
    "\t\t# Ensure consistent pairing\n",
    "\t\tfor year, bar in zip(years, citation_bars):\n",
    "\t\t\tyear_value = year.text.strip()\n",
    "\t\t\tcitation_count = bar.find('span', class_='gsc_oci_g_al').text.strip()\n",
    "\t\t\tcitations_per_year[year_value] = int(citation_count)\n",
    "\t\t\tlogging.info(f'Citation data for article ID {article_id}: , year: {year_value}, num of citation: {citations_per_year}')\n",
    "\t\t\n",
    "\t\treturn citations_per_year\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f'Error accessing {url}: {e}')\n",
    "\t\treturn {}\n",
    "\n",
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# DataFrame to store citation information\n",
    "citation_infos_df = pd.DataFrame(columns=['article_id', 'year', 'citations'])\n",
    "\n",
    "# Process each article\n",
    "for index, row in results_df.iterrows():\n",
    "\tarticle_id = row['article_id']\n",
    "\tarticle_url = row['article_url']\n",
    "\tlogging.info(f\"Accessing article ID {article_id} with URL {article_url}\")\n",
    "\n",
    "\tcitations_per_year = get_citation_data(article_id, article_url)\n",
    "\n",
    "\t# Insert citation data into DataFrame\n",
    "\tfor year, citations in citations_per_year.items():\n",
    "\t\tcitation_infos_df = pd.concat([\n",
    "\t\t\tcitation_infos_df,\n",
    "\t\t\tpd.DataFrame({'article_id': [article_id], 'year': [year], 'citations': [citations]})\n",
    "\t\t], ignore_index=True)\n",
    "\t\tlogging.info(f\"Inserting citation data for article ID {article_id}: year: {year}, num of citation: {citations}\")\n",
    "\n",
    "# Close the browser\n",
    "browser.quit()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(citation_infos_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_infos_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge citation_infos_df with results_df on article_id\n",
    "merged_df = pd.merge(citation_infos_df, results_df, on='article_id', how='inner')\n",
    "\n",
    "# Join the merged_df with df_samples on scholar_id\n",
    "final_df = pd.merge(merged_df, df_samples, on='scholar_id', how='inner')\n",
    "\n",
    "# Display the final DataFrame\n",
    "final_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analytics-qPs-RHJd-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
